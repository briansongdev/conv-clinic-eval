{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My LangChain credentials to access the vignettes dataset\n",
    "%env LANGSMITH_TRACING=true\n",
    "%env LANGSMITH_ENDPOINT=https://api.smith.langchain.com\n",
    "%env LANGSMITH_API_KEY=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\n",
    "%env LANGSMITH_PROJECT=pr-shadowy-nightgown-69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langsmith import Client\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmconstants import ClinicalLLMConstants, PatientLLMConstants\n",
    "cllm = ClinicalLLMConstants()\n",
    "pllm = PatientLLMConstants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "rows_iter = client.list_examples(dataset_name=\"casevignettes\")\n",
    "rows_arr = list(rows_iter)\n",
    "len(rows_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class ExtendedMessagesState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    clinical_instructions: Optional[str]\n",
    "    patient_instructions: Optional[str]\n",
    "    clinical_messages: Optional[str]\n",
    "    patient_messages: Optional[str]\n",
    "    termination: Optional[bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# With an LLM definition, a dataset, and an array of the max number of questions the clinician can ask the patient, this function will evaluate the accuracy and return an array\n",
    "# in the shape of the num_qs, giving accuracy for each value in num_qs\n",
    "def run_with_llm_model_as_(llm, rows_arr, num_qs):\n",
    "    acc_rates = []\n",
    "\n",
    "    for num_q in num_qs:\n",
    "        print(f\"Beginning diagnoses with LLM question limit at {num_q}!\")\n",
    "\n",
    "        workflow = StateGraph(state_schema=ExtendedMessagesState)\n",
    "\n",
    "        def call_cllm_model(state: ExtendedMessagesState):\n",
    "            clinical_history = state.get(\"clinical_messages\", [])\n",
    "            if not clinical_history:\n",
    "                clinical_history = [SystemMessage(content=cllm.get_initial_prompt(num_q))]\n",
    "\n",
    "            data_specific_instructions = state.get(\"clinical_instructions\", \"\")\n",
    "            if data_specific_instructions and len(clinical_history) <= 1:\n",
    "                clinical_history.append(HumanMessage(content=data_specific_instructions))\n",
    "            \n",
    "            # Conversational turns between clinician and patient\n",
    "            chat_messages = state.get(\"messages\", [])\n",
    "            \n",
    "            invoke_llm = clinical_history + chat_messages\n",
    "\n",
    "            response = llm.invoke(invoke_llm)\n",
    "            chat_messages.append(response)\n",
    "            state[\"clinical_messages\"] = clinical_history\n",
    "            state[\"messages\"] = chat_messages\n",
    "\n",
    "            return {\"clinical_messages\": clinical_history, \"messages\": chat_messages, \"termination\": \"Final Diagnosis\" in response.content}\n",
    "\n",
    "        def call_patient_model(state: ExtendedMessagesState):\n",
    "            patient_history = state.get(\"patient_messages\", [])\n",
    "            if not patient_history:\n",
    "                patient_history = [SystemMessage(content=pllm.get_initial_prompt())]\n",
    "            data_specific_instructions = state.get(\"patient_instructions\", \"\")\n",
    "            if data_specific_instructions and len(patient_history) <= 1:\n",
    "                patient_history.append(SystemMessage(content=data_specific_instructions))\n",
    "            \n",
    "            # Conversational turns between clinician and patient\n",
    "            chat_messages = state.get(\"messages\", [])\n",
    "            question = chat_messages[-1]\n",
    "            invoke_llm = patient_history + [HumanMessage(content=question.content)]\n",
    "\n",
    "            response = llm.invoke(invoke_llm)\n",
    "            chat_messages.append(HumanMessage(content=response.content))\n",
    "            state[\"patient_messages\"] = patient_history\n",
    "            state[\"messages\"] = chat_messages\n",
    "\n",
    "            return {\"patient_messages\": patient_history, \"messages\": chat_messages, \"termination\": state.get(\"termination\", False)}\n",
    "\n",
    "        workflow.add_node(\"clinical_model\", call_cllm_model)\n",
    "        workflow.add_node(\"patient_model\", call_patient_model)\n",
    "\n",
    "        def should_continue(state: ExtendedMessagesState) -> str:\n",
    "            if state.get(\"termination\", False):\n",
    "                return END\n",
    "            return \"patient_model\"\n",
    "\n",
    "        def should_continue_back(state: ExtendedMessagesState) -> str:\n",
    "            if state.get(\"termination\", False):\n",
    "                return END\n",
    "            return \"clinical_model\"\n",
    "\n",
    "        # Add conditional edges\n",
    "        workflow.add_edge(START, \"clinical_model\")\n",
    "        workflow.add_conditional_edges(\"clinical_model\", should_continue)\n",
    "        workflow.add_conditional_edges(\"patient_model\", should_continue_back)\n",
    "\n",
    "        memory = MemorySaver()\n",
    "        app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "        rows = rows_arr # temporary\n",
    "\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "        def execute(i):\n",
    "            try:\n",
    "                if not rows[i].outputs or \"answer\" not in rows[i].outputs:\n",
    "                    return 0\n",
    "\n",
    "                props = rows[i].inputs\n",
    "                resp = app.invoke(\n",
    "                    {\n",
    "                        \"clinical_instructions\": f\"{cllm.get_specialty(props['category'])} At the end, you will choose one diagnosis from these four options, separated by slashes:  (A) {props['choice_1']} / (B) {props['choice_2']} / (C) {props['choice_3']} / (D) {props['choice_4']}. IMPORTANT: At the end, respond ONLY with the letter of your choice â€” exactly one of A, B, C, or D, on its own line. No explanation. No parentheses. Just the letter.\",\n",
    "                        \"patient_instructions\": f\"**Case vignette**: {pllm.get_case_vignette(props['case_vignette'])}\"\n",
    "                    },\n",
    "                    config={\"configurable\": {\"thread_id\": i}, \"recursion_limit\": 300}\n",
    "                )\n",
    "                p_answer = rows[i].outputs[\"answer\"]\n",
    "                if p_answer == props[\"choice_1\"]:\n",
    "                    answer = \"A\"\n",
    "                elif p_answer == props[\"choice_2\"]:\n",
    "                    answer = \"B\"\n",
    "                elif p_answer == props[\"choice_3\"]:\n",
    "                    answer = \"C\"\n",
    "                elif p_answer == props[\"choice_4\"]:\n",
    "                    answer = \"D\"\n",
    "                else:\n",
    "                    # raise Exception(f\"Answer was not located. The prospective answer was {props['answer']}\")\n",
    "                    return 0\n",
    "                lm = resp[\"messages\"][-1].content\n",
    "                if \" A\" in lm:\n",
    "                    diagnosis = \"A\"\n",
    "                elif \" B\" in lm:\n",
    "                    diagnosis = \"B\"\n",
    "                elif \" C\" in lm:\n",
    "                    diagnosis = \"C\"\n",
    "                elif \" D\" in lm:\n",
    "                    diagnosis = \"D\"\n",
    "                else:\n",
    "                    # raise Exception(f\"Diagnosis was not located. The text involved was: {lm}\")\n",
    "                    return 0\n",
    "\n",
    "                if diagnosis == answer:\n",
    "                    return 1\n",
    "                return 0\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(execute, range(len(rows))))\n",
    "\n",
    "        success_rate = (sum(results) / len(results)) * 100 if results else 0\n",
    "        print(f\"Success Rate: {success_rate:.2f}%\")\n",
    "\n",
    "        acc_rates.append(success_rate / 100)\n",
    "\n",
    "    return acc_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "minibatch = np.random.choice(rows_arr, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_with_llm_model_as_(llm=llm, rows_arr=minibatch, num_qs=[5, 10, 15, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Llama3.2, the results were [30%, 32%, 28%, 40%] when the clinician was allowed to ask up to [5, 10, 15, 20] questions to the patient respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To try a different model, customize it here:\n",
    "my_llm = ChatOllama(\n",
    "    model=\"MODEL_NAME\"\n",
    ")\n",
    "# Set the data to be all case vignettes?\n",
    "# minibatch = rows_arr\n",
    "run_with_llm_model_as_(llm=my_llm, rows_arr=minibatch, num_qs=[5, 10, 15, 20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
